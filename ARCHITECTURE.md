# Supply Chain BDA Pipeline - Architecture

## Phase 1 Architecture (Current Implementation)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      PHASE 1: FOUNDATION                        ‚îÇ
‚îÇ                  (MongoDB + Generator + Airflow)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONTAINER: generator                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Statistical Data Generator (Python)                   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Poisson order arrivals (context-aware Œª)           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Normal lead time distribution                       ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Log-normal pricing                                  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Exponential delays                                  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Correlated attributes                               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Rate: ~1500 events/min (60s intervals)               ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                           ‚Üì                                      ‚îÇ
‚îÇ                   writes to MongoDB                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                            ‚Üì

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONTAINER: mongo (mongo:6.0)                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Hot Data Storage (Fresh Streaming Data)              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Fact Tables (Time-Series Events):                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ orders_fact       (~1000/min)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ shipments_fact    (~400/min)                     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ inventory_fact    (~40/min)                      ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Dimension Tables (Reference):                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ regions_dim       (4 records)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ dcs_dim           (5 records)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ skus_dim          (8 records)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ suppliers_dim     (4 records)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  State Collection:                                     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ inventory_state   (40 records, updated)          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Current Size: Growing at ~1 MB/min                   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Target: 300 MB (triggers archival in Phase 2)        ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                            ‚Üì

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONTAINER: airflow (apache/airflow:2.10.2)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Orchestration & Monitoring                            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  DAG: monitor_data_pipeline_phase1                     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Schedule: */5 * * * * (every 5 minutes)              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Tasks:                                                 ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    1. check_mongo_connection                           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    2. monitor_data_volume                              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    3. check_data_freshness                             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    4. log_system_status                                ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  UI: http://localhost:8080                             ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Phase 2 Architecture (Planned - Next Implementation)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PHASE 2: ANALYTICS + ARCHIVING                     ‚îÇ
‚îÇ         (+ Spark + HDFS + Redis + Archival Logic)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                    [Generator] (existing)
                         ‚Üì
                    [MongoDB] (existing)
                         ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                                  ‚Üì
   [Airflow DAG 1]                   [Airflow DAG 2]
   archive_to_hdfs                   compute_kpis
   (when size > 300MB)               (every 1-2 min)
        ‚Üì                                  ‚Üì
        ‚Üì                            [Spark Master]
        ‚Üì                                  ‚Üì
        ‚Üì                            [Spark Worker]
        ‚Üì                                  ‚Üì
        ‚Üì                            Read MongoDB
        ‚Üì                            Join fact √ó dims
        ‚Üì                            Compute KPIs
        ‚Üì                                  ‚Üì
   [HDFS NameNode]                      [Redis]
        ‚Üì                            (cache KPIs)
   [HDFS DataNode]                        ‚Üì
   (Parquet files)                  [Dashboard]
   Partitioned:                     (Phase 3)
   /archive/
     event_date=2025-12-25/
       hour=14/
         part-00000.parquet

   + Metadata (JSON):
   {
     "archive_id": "...",
     "date_range": [...],
     "record_count": 150000,
     "size_mb": 320
   }
```

---

## Detailed Layer Breakdown

### Layer 1: Data Ingestion ‚úÖ (Phase 1)
**Component:** `generator` container  
**Technology:** Python 3.11 + NumPy  
**Function:** Generate streaming supply chain events using statistical models  
**Output:** MongoDB writes (1500 events/min)

### Layer 2: Fresh Data Storage ‚úÖ (Phase 1)
**Component:** `mongo` container  
**Technology:** MongoDB 6.0 (ARM64 native)  
**Function:** Hot data store for recent events (< 300 MB)  
**Schema:** Star schema (3 fact tables, 4 dimension tables)

### Layer 3: Metadata Storage ‚è≥ (Phase 2)
**Component:** HDFS `/archive/metadata/`  
**Technology:** JSON manifests in HDFS  
**Function:** Track archival batches (date ranges, counts, sizes)

### Layer 4: Archive Storage ‚è≥ (Phase 2)
**Component:** HDFS NameNode + DataNode  
**Technology:** Hadoop 3.2.1 (Rosetta emulation on M1)  
**Function:** Cold storage for data > 300 MB  
**Format:** Parquet (columnar, compressed)

### Layer 5: Staging & Transformation ‚è≥ (Phase 2)
**Component:** Spark jobs  
**Function:** Clean, validate, enrich data before analytics  
**Operations:** Null handling, schema validation, derived columns

### Layer 6: Analytics (Spark SQL / OLAP) ‚è≥ (Phase 2)
**Component:** Spark Master + Worker  
**Function:** Compute KPIs via multi-table joins  
**Queries:**
- WHERE (time-windowed)
- GROUP BY (sku, dc, region)
- HAVING (threshold filters)
- Joins (fact √ó dimensions)

### Layer 7: Cache ‚è≥ (Phase 2)
**Component:** Redis 7  
**Function:** Store pre-aggregated KPIs (sub-second reads)  
**TTL:** 60 seconds (refreshed by Spark)

### Layer 8: BI Dashboard ‚è≥ (Phase 3)
**Component:** Streamlit  
**Function:** Live visualizations (updates every 60s)  
**Data Source:** Reads from Redis only (no MongoDB queries)

---

## Orchestration Flow

### Phase 1 (Current)
```
[Airflow Scheduler]
    ‚Üì
    ‚îú‚îÄ> Every 5 min: monitor_data_pipeline_phase1
    ‚îÇ       ‚îú‚îÄ> Check MongoDB health
    ‚îÇ       ‚îú‚îÄ> Monitor data volume
    ‚îÇ       ‚îú‚îÄ> Check data freshness
    ‚îÇ       ‚îî‚îÄ> Log system status
    ‚îî‚îÄ> (Generator runs independently, not DAG-managed)
```

### Phase 2 (Planned)
```
[Airflow Scheduler]
    ‚Üì
    ‚îú‚îÄ> Every 5 min: check_mongodb_size
    ‚îÇ       ‚îî‚îÄ> If size > 300 MB:
    ‚îÇ           ‚îî‚îÄ> Trigger: archive_to_hdfs
    ‚îÇ               ‚îú‚îÄ> spark-submit archive_mongo_to_hdfs.py
    ‚îÇ               ‚îú‚îÄ> Move old data to HDFS (Parquet)
    ‚îÇ               ‚îú‚îÄ> Write metadata JSON
    ‚îÇ               ‚îî‚îÄ> Delete archived data from MongoDB
    ‚îÇ
    ‚îî‚îÄ> Every 1 min: compute_kpis
            ‚îî‚îÄ> spark-submit compute_minute_kpis.py
                ‚îú‚îÄ> Read MongoDB (last 15 min)
                ‚îú‚îÄ> Join orders √ó inventory √ó skus √ó dcs √ó regions
                ‚îú‚îÄ> Compute 5 KPIs
                ‚îî‚îÄ> Write to Redis cache
```

---

## Data Flow Diagram

```
REAL-TIME PATH (Hot Data):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇGenerator‚îÇ --> ‚îÇ MongoDB ‚îÇ --> ‚îÇ Spark ‚îÇ --> ‚îÇ  Redis   ‚îÇ
‚îÇ(Python) ‚îÇ     ‚îÇ (Hot)   ‚îÇ     ‚îÇ(OLAP) ‚îÇ     ‚îÇ (Cache)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                     ‚Üì
                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                              ‚îÇDashboard ‚îÇ
                                              ‚îÇ(Streamlit‚îÇ
                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

ARCHIVAL PATH (Cold Data):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Airflow ‚îÇ --> ‚îÇ  Spark  ‚îÇ --> ‚îÇ HDFS (Parquet)   ‚îÇ
‚îÇ(Trigger)‚îÇ     ‚îÇ(Archive)‚îÇ     ‚îÇ + Metadata (JSON)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üë
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ MongoDB ‚îÇ
                ‚îÇ (Prune) ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## KPI Computation (Phase 2 Design)

### KPI 1: Total Inventory Level
```sql
SELECT sku_id, dc_id, SUM(on_hand_qty) AS total_inventory
FROM inventory_fact
WHERE inventory_ts >= NOW() - INTERVAL 15 MINUTES
GROUP BY sku_id, dc_id
```

### KPI 2: Stockout Risk (Days Until Stockout)
```sql
SELECT 
    i.sku_id, 
    i.dc_id,
    AVG(i.on_hand_qty) AS avg_stock,
    SUM(o.quantity) / 15.0 AS units_per_min,
    (avg_stock / NULLIF(units_per_min, 0)) / 1440 AS days_to_stockout
FROM inventory_fact i
LEFT JOIN orders_fact o 
    ON i.sku_id = o.sku_id 
    AND i.dc_id = o.dc_id
WHERE i.inventory_ts >= NOW() - INTERVAL 15 MINUTES
  AND o.order_ts >= NOW() - INTERVAL 15 MINUTES
GROUP BY i.sku_id, i.dc_id
HAVING days_to_stockout < 7
```

### KPI 3: Supplier Lead Time Performance
```sql
SELECT 
    s.supplier_id,
    AVG(sf.lead_time_days) AS avg_lead_time,
    STDDEV(sf.lead_time_days) AS lead_time_variance
FROM shipments_fact sf
JOIN suppliers_dim s ON sf.supplier_id = s.supplier_id
WHERE sf.shipment_ts >= NOW() - INTERVAL 30 MINUTES
GROUP BY s.supplier_id
```

### KPI 4: DC Utilization Rate
```sql
SELECT 
    d.dc_id,
    SUM(i.on_hand_qty * sk.storage_m3) AS current_volume_m3,
    d.capacity_m3,
    (current_volume_m3 / d.capacity_m3) AS utilization_rate
FROM inventory_fact i
JOIN dcs_dim d ON i.dc_id = d.dc_id
JOIN skus_dim sk ON i.sku_id = sk.sku_id
WHERE i.inventory_ts >= NOW() - INTERVAL 15 MINUTES
GROUP BY d.dc_id
HAVING utilization_rate > 0.85
```

### KPI 5: Order Fulfillment Rate
```sql
SELECT 
    r.region_id,
    COUNT(*) AS total_orders,
    SUM(o.order_value) AS total_revenue
FROM orders_fact o
JOIN dcs_dim d ON o.dc_id = d.dc_id
JOIN regions_dim r ON d.region_id = r.region_id
WHERE o.order_ts >= NOW() - INTERVAL 15 MINUTES
GROUP BY r.region_id
```

---

## Resource Allocation Summary

| Phase | Services | Total Memory | Docker Limit Needed |
|-------|----------|--------------|---------------------|
| **Phase 1** | 3 (Mongo, Generator, Airflow) | ~2.3 GB | 4 GB |
| **Phase 2** | 8 (+ Spark, HDFS, Redis) | ~5.0 GB | 6 GB |
| **Phase 3** | 9 (+ Dashboard) | ~5.3 GB | 6 GB |

**macOS M1 8GB:** Safe (leaves 2-3 GB for OS)

---

## Technology Justification

### Why MongoDB?
- ‚úÖ Fast writes (high insert throughput)
- ‚úÖ Flexible schema (easy to iterate)
- ‚úÖ Native time-series support
- ‚úÖ ARM64 compatible

### Why Spark?
- ‚úÖ Distributed processing (scales horizontally)
- ‚úÖ SQL-like syntax (meets OLAP requirement)
- ‚úÖ Native Parquet support
- ‚úÖ MongoDB connector available

### Why HDFS?
- ‚úÖ Assignment requirement (mandatory)
- ‚úÖ Industry standard for big data archival
- ‚úÖ Fault-tolerant (replication)
- ‚ö†Ô∏è Requires Rosetta on M1 (acceptable trade-off)

### Why Redis?
- ‚úÖ Sub-millisecond reads (dashboard performance)
- ‚úÖ Decouples visualization from compute
- ‚úÖ TTL support (auto-expire stale data)

### Why Airflow?
- ‚úÖ Assignment requirement (mandatory)
- ‚úÖ Visual DAG editor (easy debugging)
- ‚úÖ Cron-like scheduling
- ‚úÖ Retry logic & monitoring

### Why Streamlit?
- ‚úÖ Python-native (easy integration)
- ‚úÖ Auto-refresh support
- ‚úÖ Interactive filters
- ‚úÖ Fast development

---

## Security Considerations

### Current (Phase 1)
- MongoDB: Username/password authentication
- Airflow: Admin user with strong password
- Docker network: Default bridge (isolated from host)

### Future Enhancements (Production)
- TLS/SSL for MongoDB connections
- Airflow RBAC (role-based access control)
- Secrets management (Docker secrets or Vault)
- HDFS Kerberos authentication

---

## Performance Optimization Strategies

### Already Implemented (Phase 1)
‚úÖ MongoDB indexes on timestamps  
‚úÖ Bulk inserts (ordered=False for parallelism)  
‚úÖ Conservative memory limits  
‚úÖ WiredTiger cache limit (512MB)

### Planned (Phase 2)
- Spark broadcast joins (for small dimensions)
- Parquet columnar storage (efficient compression)
- Partition pruning (date/hour partitions)
- Redis pipeline batching
- Early aggregation before joins

---

## Monitoring & Observability

### Phase 1 (Current)
- Airflow DAG logs
- Docker logs (`docker logs <container>`)
- MongoDB shell queries

### Phase 2 (Planned)
- Spark UI (port 8081)
- HDFS NameNode UI (port 9870)
- Airflow task logs with XCom
- Custom metrics in Redis

### Phase 3 (Future)
- Prometheus + Grafana (optional)
- Custom alerting (email/Slack on failures)

---

## Testing Strategy

### Unit Tests
- Statistical model functions (models.py)
- Data validation logic

### Integration Tests
- End-to-end data flow (generator ‚Üí MongoDB ‚Üí Spark ‚Üí Redis)
- Archival process (MongoDB ‚Üí HDFS)

### Performance Tests
- Sustained load (24-hour continuous generation)
- Memory leak detection (`docker stats`)

### Manual Tests
- Dashboard visual inspection
- Airflow DAG execution
- MongoDB query performance

---

## Next Steps Checklist

### Before Phase 2
- [ ] Let Phase 1 run for 2-5 hours
- [ ] Verify database reaches 50-100 MB
- [ ] Confirm no memory/CPU issues
- [ ] Review Airflow logs for any warnings

### Phase 2 Implementation
- [ ] Add Spark services to docker-compose.yml
- [ ] Add HDFS services (with Rosetta enabled)
- [ ] Add Redis service
- [ ] Implement archive_mongo_to_hdfs.py (Spark job)
- [ ] Implement compute_minute_kpis.py (Spark job)
- [ ] Create new Airflow DAGs for archiving + analytics
- [ ] Test archival process manually
- [ ] Verify KPIs appear in Redis

---

## Questions & Answers

**Q: Why not use Kafka for streaming?**  
A: Controlled generation (not event-driven) simplifies testing and meets requirements without added complexity.

**Q: Why SQLite for Airflow (not PostgreSQL)?**  
A: SequentialExecutor is sufficient for single-node. SQLite reduces overhead.

**Q: Can I run this on x86 Linux?**  
A: Yes! Remove Rosetta requirement, use native Hadoop images.

**Q: What if I exceed 8GB RAM?**  
A: Reduce EVENTS_PER_MINUTE in generator, lower Spark worker memory, or pause services selectively.

**Q: How to reset and start fresh?**  
A: `docker compose down -v` (deletes all data volumes)

---

## Conclusion

Phase 1 establishes a **production-grade foundation** for real-time Big Data Analytics:

‚úÖ Statistically realistic data generation  
‚úÖ Proper schema design (star schema)  
‚úÖ Orchestration & monitoring  
‚úÖ Resource-efficient (2.3 GB)  
‚úÖ ARM64 native (no emulation)  
‚úÖ Ready for Phase 2 expansion

**The hard part is done. Phases 2-3 build on this solid base!** üöÄ

